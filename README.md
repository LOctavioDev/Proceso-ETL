# ğŸš€ Proceso ETL con Diferentes OrÃ­genes o Fuentes de Datos (Data Source)

---

### ğŸ‘¨â€ğŸ“ Autor del Proyecto

- **Nombre:** Luis Octavio LÃ³pez MartÃ­nez  
- **MatrÃ­cula:** 220096  
- **Carrera:** IngenierÃ­a en Desarrollo y GestiÃ³n de Software  
- **Materia:** ExtracciÃ³n de Conocimiento en Bases de Datos  

---

## ğŸ“š DescripciÃ³n General

El estudiante aplicarÃ¡ los conceptos del proceso **ETL** (**ExtracciÃ³n, TransformaciÃ³n y Carga de Datos**), utilizando fuentes de datos variadas como APIs pÃºblicas, bases de datos relacionales (PostgreSQL) y no relacionales (MongoDB), para demostrar un flujo completo de anÃ¡lisis de datos.

AdemÃ¡s, se emplearÃ¡n herramientas de visualizaciÃ³n interactivas como **Dash** y **Bokeh** para representar grÃ¡ficamente los resultados.

---

## ğŸ³ Uso de Docker y PowerShell

- Se utilizaron **contenedores Docker** para levantar las bases de datos PostgreSQL y MongoDB, garantizando entornos reproducibles y portables.
- Los comandos para manejar los contenedores y ejecutar los scripts se automatizaron mediante **PowerShell** con `Invoke-Command` y scripts `.ps1`, facilitando la ejecuciÃ³n desde Windows.
- Esto permite levantar, detener y monitorear fÃ¡cilmente los servicios de base de datos sin configuraciones complejas en la mÃ¡quina local.

---

## ğŸ“¸ VisualizaciÃ³n de Contenedores Docker

![Contenedores Docker levantados](assets/docker.png)

---

## ğŸ› ï¸ TecnologÃ­as Utilizadas

<p align="left">
  <img src="https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white"/>
  <img src="https://img.shields.io/badge/Jupyter-F37626?style=for-the-badge&logo=jupyter&logoColor=white"/>
  <img src="https://img.shields.io/badge/PostgreSQL-4169E1?style=for-the-badge&logo=postgresql&logoColor=white"/>
  <img src="https://img.shields.io/badge/MongoDB-4EA94B?style=for-the-badge&logo=mongodb&logoColor=white"/>
  <img src="https://img.shields.io/badge/Dash-000000?style=for-the-badge&logo=plotly&logoColor=white"/>
  <img src="https://img.shields.io/badge/Bokeh-1A1A1A?style=for-the-badge&logo=bokeh&logoColor=white"/>
  <img src="https://img.shields.io/badge/Flask-000000?style=for-the-badge&logo=flask&logoColor=white"/>
  <img src="https://img.shields.io/badge/Pandas-150458?style=for-the-badge&logo=pandas&logoColor=white"/>
  <img src="https://img.shields.io/badge/Plotly-3F4F75?style=for-the-badge&logo=plotly&logoColor=white"/>
  <img src="https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white"/>
  <img src="https://img.shields.io/badge/PowerShell-012456?style=for-the-badge&logo=powershell&logoColor=white"/>
</p>

---

## ğŸ§© Contenido

Este proyecto se divide en tres partes:

1. **Base de Datos Relacional**: ExtracciÃ³n desde PostgreSQL y visualizaciÃ³n con Dash.  
2. **API PÃºblica (COVID-19)**: Consumo de datos en tiempo real y visualizaciÃ³n con Flask + Bokeh.  
3. **Base de Datos No Relacional (MongoDB)**: ExtracciÃ³n desde Open-Meteo, almacenamiento y visualizaciÃ³n con Dash.

Cada mÃ³dulo incluye su propio script, documentaciÃ³n y visualizaciÃ³n.

---
"""

## Actividades

## ğŸ—ƒï¸ 1. Base de Datos Relacional (PostgreSQL)

### ğŸ“„ Archivo principal: `scripts/p1.py`

### ğŸ§  DescripciÃ³n del Script

Este script se conecta a una base de datos relacional **PostgreSQL** utilizando la librerÃ­a `SQLAlchemy`. Su propÃ³sito es extraer, transformar y visualizar artÃ­culos de Wikipedia almacenados en la tabla `public.articles`.

---

### ğŸ”„ Flujo de Trabajo:

1. **ConexiÃ³n a la base de datos:**
   ```python
   from sqlalchemy import create_engine
   engine = create_engine("postgresql+psycopg2://postgres:postgres@localhost:5434/wikipedia")
   ```

2. **ExtracciÃ³n de datos:**
   ```python
   import pandas as pd
   query = "SELECT id, url, title, content FROM public.articles"
   df = pd.read_sql(query, engine)
   ```

3. **TransformaciÃ³n de datos:**
   - EliminaciÃ³n de valores nulos.
   - Filtrado por longitud del contenido.
   - Ordenamiento para mostrar los artÃ­culos mÃ¡s extensos.

4. **VisualizaciÃ³n con Dash y Plotly:**
   - Histograma de longitud de artÃ­culos.
   - Tabla y grÃ¡fica de los 10 artÃ­culos mÃ¡s largos.

---

### ğŸ“Š Resultados Visuales

| Histograma de longitud de artÃ­culos | Top 10 artÃ­culos mÃ¡s largos |
|:-----------------------------------:|:---------------------------:|
| ![Histograma](assets/p1.png)         | ![Top 10](assets/p11.png)   |


## ğŸŒ 2. API PÃºblica (COVID-19)

### ğŸ“„ Archivo principal: `scripts/p2.py`

### ğŸ§  DescripciÃ³n del Script

Este script consume una **API pÃºblica de COVID-19** y visualiza los casos confirmados por paÃ­s en un mapa interactivo. Utiliza **Flask** como servidor web y **Bokeh** para renderizar visualizaciones geogrÃ¡ficas dinÃ¡micas.

---

### ğŸ”„ Flujo de Trabajo:

1. **Consumo de la API**:
   - Se obtienen los datos actualizados desde `https://disease.sh/v3/covid-19/countries`.
   - Se transforma la respuesta en un `DataFrame` con `pandas`.

2. **IntegraciÃ³n con GeoJSON mundial**:
   - Se descarga un archivo GeoJSON desde GitHub.
   - Se integran los datos de COVID por paÃ­s mediante el cÃ³digo ISO3.

3. **VisualizaciÃ³n con Bokeh**:
   - Se genera un mapa mundial donde el color representa la cantidad de casos.
   - Se personaliza con `HoverTool` para mostrar informaciÃ³n por paÃ­s.

4. **Renderizado con Flask**:
   - El resultado se incrusta dinÃ¡micamente en HTML usando `components()` de Bokeh.
   - La visualizaciÃ³n se muestra al acceder a la ruta principal `/`.

---

### ğŸ—ºï¸ VisualizaciÃ³n

![VisualizaciÃ³n API COVID](assets/p3.png)

---

### ğŸ§© LibrerÃ­as Utilizadas

- `requests`: para consumir la API REST
- `pandas`: para manejar y transformar los datos
- `bokeh`: para crear visualizaciones interactivas
- `flask`: servidor web para renderizar el HTML
- `json`: manejo de estructuras GeoJSON

---


---

### ğŸ’¡ Notas

- El script obtiene **datos en vivo**, asÃ­ que los nÃºmeros cambian con el tiempo.
- El cÃ³digo ISO3 es clave para integrar correctamente los datos en el mapa.
- El mapa estÃ¡ centrado automÃ¡ticamente y se puede hacer zoom.

---
  

## â˜ï¸ 3. Extra: Base de Datos No Relacional (MongoDB)

### ğŸ”„ ExtracciÃ³n, transformaciÃ³n y carga (ETL)

#### ğŸ“„ Archivo: `scripts/pe.py`

Este script obtiene datos histÃ³ricos de clima para varias ciudades mexicanas desde la **API pÃºblica de Open-Meteo**, los transforma y los carga en una colecciÃ³n de **MongoDB** (`clima.historico`).

---

### ğŸ§  Proceso de ETL:

1. **ExtracciÃ³n**:
   - Se hace una peticiÃ³n HTTP a la API de Open-Meteo por ciudad y por fechas.
   - Se obtienen variables como temperatura mÃ¡xima/mÃ­nima, lluvia, viento y UV.

2. **TransformaciÃ³n**:
   - Se formatea cada entrada como documento JSON por dÃ­a y por ciudad.
   - Se normalizan los nombres y estructuras de los datos.

3. **Carga**:
   - Cada documento se inserta en la colecciÃ³n `historico` dentro de la base de datos `clima`.

---

### ğŸ§ª VisualizaciÃ³n y anÃ¡lisis

#### ğŸ“„ Archivo: `scripts/pm.py`

Una vez cargados los datos en MongoDB, este segundo script se conecta a la base, limpia los datos y genera visualizaciones con **Dash** y **Plotly**.

---

### ğŸ“ˆ VisualizaciÃ³n:

- Se muestra un dropdown interactivo para seleccionar ciudad.
- Se grafican las temperaturas mÃ¡ximas y mÃ­nimas diarias.
- Se usa `plotly_dark` para un tema oscuro moderno.

---

### ğŸ—ºï¸ Resultados Visuales

| Temperaturas por ciudad (lÃ­nea) |
|---------------------------------|
| ![VisualizaciÃ³n MongoDB](assets/p2.png) |
| ![VisualizaciÃ³n MongoDB](assets/p22.png) |

---

### ğŸ’¡ Notas

- La API de Open-Meteo no requiere autenticaciÃ³n y soporta fechas pasadas con precisiÃ³n diaria.
- MongoDB permite almacenar documentos por ciudad y fecha sin estructura rÃ­gida.
- Ideal para practicar un flujo completo de **API â†’ procesamiento â†’ base de datos â†’ visualizaciÃ³n**.

---

### ğŸ§© LibrerÃ­as Utilizadas

- `requests`: consumir API de clima
- `pymongo`: conexiÃ³n y carga de datos a MongoDB
- `dash`, `plotly.express`: visualizaciÃ³n web interactiva
- `pandas`: procesamiento de datos
- `json`: conversiÃ³n estructurada de la API

---

## Herramientas Utilizadas
- **Jupyter Notebook**
- **Python**
- **LibrerÃ­as:** pandas, sqlalchemy, requests, pymongo, dash, plotly, flask, bokeh, entre otras.

## Objetivo
Desarrollar habilidades prÃ¡cticas en la manipulaciÃ³n de datos provenientes de diferentes fuentes, aplicando el proceso ETL completo y utilizando herramientas modernas de anÃ¡lisis de datos.


